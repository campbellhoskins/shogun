# Shogun

## Why This Project Exists

Shogun is a demonstration project being built to prove technical capability to Arjun Chopra, a Silicon Valley VC (Floodgate partner) who is assembling a team to build an AI-powered travel industry roll-up. The venture has three interlocking entities:

- **O1** — A hybrid PE/VC investment firm (similar to General Catalyst's model) that acquires travel management companies already entrenched in the industry.
- **Spotnana** — A modern cloud-native travel infrastructure platform ($100M+ raised) that replaces legacy GDS systems with APIs. Acquired companies get migrated onto Spotnana.
- **Shogun (the AI task force)** — A "Palantir for Travel" that deploys Forward Deployed Engineers into acquired companies to automate their workflows using AI. This is the entity I'm building toward joining.

The business thesis: Travel management companies run on **30% manual workflows**. A $600M revenue TMC at 10% margin earns $60M profit — but if you automate even half the manual work, margin jumps to 15%+ ($90M). The play is to acquire these companies (which have regulatory moats like IATA/ARC accreditation), migrate them onto Spotnana, and deploy AI agents to automate operations.

## What I'm Demonstrating

An engineer they hired from Amazon impressed Arjun by spending a weekend building an agent that ingests travel compliance data and constructs an ontology graph from it. I need to demonstrate the same kind of initiative — but go further. Instead of just parsing a document into a graph, this project shows the full pipeline:

1. **Ingest** a real corporate duty of care policy document (PDF)
2. **Extract** a complete ontology graph — entities, relationships, attributes — that captures every rule, threshold, role, definition, and procedure
3. **Reason** over the graph using an AI agent with tool-use that traverses the graph programmatically (not just dumping text into a prompt)
4. **Evaluate** the quality of both the graph and the agent's answers against a ground-truth test set

The sophistication matters because this mirrors what Shogun would actually deploy: take messy, unstructured policy documents from acquired TMCs and turn them into structured knowledge that agents can act on. If I can demonstrate this on duty of care policies, the same pattern extends to booking rules, fare rules, expense policies, and the PNR disruption workflows that are the core business opportunity.

## The Bar

Arjun specifically praised the Amazon engineer's initiative. The bar is not "interesting prototype" — it's "this person clearly understands the domain, the technical challenges, and can build production-quality tooling." Every piece of this project should reflect that standard.

## Project Structure Conventions

### PDF Parsers

We maintain two PDF-to-markdown parsers. **Parser 1 is the active parser** used by the pipeline (`src/pdf_parser.py`).

- `src/pdf_parser.py` — Parser 1 (custom heuristic, font-metric analysis via raw PyMuPDF). Handles bold ALL-CAPS heading detection, page number stripping, repeated header/footer removal, TOC dot-filler removal, cover page logic, definition formatting, and paragraph joining. **This is imported by `src/main.py`.**
- `src/pdf_parser_1.py` — Named copy of Parser 1 for reference.
- `src/pdf_parser_2.py` — Parser 2 (pymupdf4llm wrapper). Better at tables, links, and italic preservation, but misses body-size bold headings and does not strip noise.

### Parsed Output Organization

Each parser writes its markdown output to its own subfolder under `data/`:

- `data/parser_1/` — Markdowns generated by Parser 1
- `data/parser_2/` — Markdowns generated by Parser 2

Source PDFs live in `data/` root. Do not mix parser outputs into the same folder.

### Result Storage Conventions

Every pipeline run that makes API calls **must** save its outputs to `results/`. No LLM-generated data should exist only in console output or in-memory. If an API was called and tokens were spent, the result has a file.

#### Directory Layout

```
results/
├── runs/                              # One subdirectory per pipeline run
│   ├── {timestamp}_{policy_name}/     # Run ID = UTC timestamp + sanitized policy name
│   │   ├── run_meta.json              # Run metadata, timings, counts, anchoring stats
│   │   ├── sections.json              # Stage 1: LLM segmentation output
│   │   ├── extractions.json           # Stage 2: per-section extraction (pre-merge)
│   │   ├── ontology.json              # Stage 3: final merged OntologyGraph (reloadable)
│   │   ├── entities.json              # Entities grouped by type (human-readable)
│   │   └── relationships.json         # Relationships grouped by type (human-readable)
│   └── ...
└── latest.txt                         # Contains the run_id of the most recent run
```

#### File Contracts

Each file in a run directory has a fixed schema. Do not add ad-hoc files or change these schemas without updating both `src/results.py` and this section.

| File | Purpose | Schema Owner |
|------|---------|--------------|
| `run_meta.json` | Pipeline config, timings, entity/relationship counts, source anchoring quality stats | `save_run()` in `src/results.py` |
| `sections.json` | Array of sections: header, section_number, level, char_count, enumerated_lists | `save_run()` — sourced from `DocumentSection` model |
| `extractions.json` | Array of per-section results: section info + raw entities/relationships before merge | `save_run()` — sourced from `SectionExtraction` model |
| `ontology.json` | Full `OntologyGraph.model_dump()` — reloadable via `OntologyGraph(**data)` | `save_run()` — sourced from `OntologyGraph` model |
| `entities.json` | Entities grouped by type with id, name, description, attributes, source_section, source_text | `save_run()` — derived view for human reading |
| `relationships.json` | Relationships grouped by type with resolved entity names | `save_run()` — derived view for human reading |

#### Rules

1. **Every pipeline run auto-saves.** The `extract_ontology()` function in `src/pipeline.py` calls `save_run()` at the end of every execution. No manual save step.
2. **Never lose API results.** If a new pipeline stage is added (e.g., cross-section inference, verification), its LLM output must be added as a new file in the run directory and registered in `save_run()`.
3. **Run IDs are immutable.** Format: `YYYY-MM-DDTHH-MM-SS_{policy_name}`. Once a run is saved, its directory is never overwritten — each run gets a unique timestamp.
4. **`ontology.json` is the source of truth.** It contains the complete serialized graph and can reconstruct the full `OntologyGraph` object. All other files are derived views for convenience.
5. **Reload without re-running.** Use `load_latest_ontology()` or `load_run(run_id)` from `src/results.py` to reload any previous run's graph without making API calls.
6. **When adding a new pipeline stage**, add a corresponding JSON file to the run directory (e.g., `cross_section.json`, `verification.json`), update `save_run()` to write it, and update `load_run()` to read it. Document the new file in this table.

#### What Goes Where

| Data Type | Where It Lives |
|-----------|---------------|
| Source policy PDFs | `data/` |
| PDF-to-markdown conversions | `output/parsed/` |
| Q&A test sets | `data/*.qa.json` |
| Pipeline run outputs (entities, relationships, ontology) | `results/runs/{run_id}/` |
| Evaluation results (agent Q&A scoring) | `output/eval_results.json` |
| Graph visualizations (interactive HTML) | `output/graph.html` |
| Saved graphs (legacy format from `build_graph.py`) | `output/graphs/` |

## Core Principles

- **Accuracy over cost.** Never accept a worse outcome to save tokens or reduce API calls. If the agent needs more iterations, let it iterate.
- **Enterprise-grade quality.** Every decision should be evaluated as: "Would this hold up at scale with high-stakes compliance documents where errors have real consequences?"
- **No shortcuts.** If a more robust solution exists — even if harder to implement — that is the correct choice.
- **Correctness first, then performance.** A fast wrong answer is worthless. A slower correct answer is the standard.
